{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is composed by 6494 headlines, the mean length is 70 characters and the maximum length is 168 characters. I decided to pad all the headlines to the same length (168) with the special character \"~\" and to add at the begining the special character \"^\". This is because this implementation only supports fixed length chunks and in order to give to the model the possibility to learn the semantic rules of an italian phrase it should observe one complete phrase as training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n"
     ]
    }
   ],
   "source": [
    "#load the dateset\n",
    "import pandas as pd\n",
    "\n",
    "#load the dataset, no header\n",
    "df = pd.read_csv('lercio_headlines.csv', header=None)\n",
    "\n",
    "#record with the maximum number of characters\n",
    "max = df[0].str.len().max()\n",
    "print(max)\n",
    "\n",
    "# pad the headlines with special characters to make them all the same length\n",
    "df[0] = df[0].str.pad(max, side='right', fillchar='~')\n",
    "\n",
    "# insert a special character at the beginning of each headline\n",
    "df[0] = \"^\" + df[0]\n",
    "\n",
    "# create a txt file with the headlines\n",
    "with open('lercio_padded.txt', 'w') as f:\n",
    "    for line in df[0]:\n",
    "        f.write(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original experiments with this model have been made on the Shaekespeare dataset that is approximately 3 times smaller than the lercio dataset (without padding), since the original experiments were performed with 2000 epoch, i used 6000 epoch of training as baseline.\n",
    "\n",
    "The other parameters that I played with are the hidden size of the gru units and the number of layer, in addition to the temperature in the generation phase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's difficult to systematically asses the quality of the generated headlines, in particular the semantic accuracy. Concerning the correctness of the single words, I used a dataset found on Github (https://github.com/napolux/paroleitaliane). The \"parole.txt\" file contains an list of words in italian (almost complete in my opinion, around 1 milion words), including: compound words, names, surnames, cities and locations, verbs, adjectives, adverbs, etc.\n",
    "\n",
    "The idea is to use that file to calculate the percetage of correct words generate by the model with different hidden size, number of layers and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "952734"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open the file \"parole.txt\" and read it creating a dictionary\n",
    "\n",
    "real_words = {}\n",
    "with open('./dataset/parole.txt', 'r') as f:\n",
    "    for p in f:\n",
    "        real_words[p.strip().lower()] = True\n",
    "\n",
    "len(real_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate import generate\n",
    "\n",
    "def percetage_correct_words(real_words, model_path, temperature, num_titles, len_titles):\n",
    "    total_words = 0\n",
    "    wrong_words = 0\n",
    "    for i in range(num_titles):\n",
    "        title = generate(model_path, temperature, len_titles)\n",
    "        # remove the ^ character\n",
    "        title = title[1:]\n",
    "        # remove the ~ characters\n",
    "        title = title.replace('~', '')\n",
    "        # for each word in the title\n",
    "        for word in title.split():\n",
    "            # if the word is not in the dictionary\n",
    "            if word.lower() not in real_words:\n",
    "                wrong_words += 1\n",
    "            total_words += 1\n",
    "    \n",
    "    return 1 - wrong_words/total_words          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8213157138753232"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "percetage_correct_words(real_words, './models/lercio_E6000_H200_L1.pt', 0.3, 100, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ispr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
